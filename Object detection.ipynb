{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "\n",
    "- https://lilianweng.github.io/lil-log/2017/10/29/object-recognition-for-dummies-part-1.html\n",
    "- https://github.com/tatsuyah/vehicle-detection\n",
    "- https://github.com/qqwweee/keras-yolo3\n",
    "- https://github.com/ablacklama/Vehicle-Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. YOLO alogirthm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 YOLO3 architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. CNN, Resdiual Blocks, Skip Connections, Upsampling\n",
    "2. Object detection, bounding box regression, IoU and non-maximum suppression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- YOLOv3 contains 53 convolutional layers (stride 2), each followed by batch normalization layer and Leaky ReLU activation.\n",
    "- If the stride of the network is 32, then an input image of size 416x416 will yield an output of size 13x13 (416/32=13)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The input is a batch of images of shape (m, 416, 416, 3)\n",
    "- The output is a list of bouding boxes along with the recognized classes. Each bounding box is represented by 6 numbers (pc, bx, by, bh, bw, c). If we expand c into an 80-dimensional vector, each bouding box is then representd by 85 numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In YOLO, the prediction is done by using a convolutional layer which uses 1x1 convolutions. So, the size of the prediction map is  exactly the size of the feature map before it.\n",
    "- If we have (Bx(5+C)) entries in the feature map. B - number of bouding boxes each cell can predict, each of these B bouding boxed may specialize in detecting a certain kind of object.\n",
    "- Each of the bouding boxed have 5 + C attributes, which describe the center coordinates, the dimensions, the objectness score and C class confidences for each bouding box. \n",
    "- YOLOv3 predicts 3 bouding boxes for every cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$b_x = \\sigma{t_x} + c_x$$\n",
    "$$b_y = \\sigma{t_y} + c_y$$\n",
    "$$b_w = p_w e^{t_w}$$\n",
    "$$b_h = p_h e^{t_h}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- bx, by, bw, bh are the x, y center coordinates, width and height.\n",
    "- tx, ty, tw, th is the network output.\n",
    "- cx, cy are the top-left coordinats of the grid \n",
    "- pw and ph are anchors dimensions for the box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Center Coordinates\n",
    "- Center coordinates prediction are running through a sigmoid function, so the output is limited from 0 to 1.\n",
    "- YOLO3 predicts offsets which are:\n",
    "    - Relative to the top left corner of the grid cell which is predicting the object\n",
    "    - Normalized by the dimensions of the cell from the feature map, which is, 1.\n",
    "\n",
    "\n",
    "Dimensions of the Bouding Box\n",
    "- Dimensions of the bounding box are predicted by applying a log-space transformation to the output and then multipying with an anchor\n",
    "\n",
    "Objectness Score and Class Confidences \n",
    "- The objectness score is also passed through a sigmoid, as it is to be interpreted as a probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- YOLOv3 makes prediction across 3 different scales. The detection layer is used make detection at feature maps of three differen sizes, having strides 32, 16, 8 respectively. \n",
    "- With an input of 416x416, we make detections on scales 13x13, 26x26, 52x52.\n",
    "- At each scale, each cell predicts 3 bouding boxes using 3 anchors, makeing the total number of anchots used 9.\n",
    "- Output Processing (Filtering with a threshold on class scores)\n",
    "    - For an image of size (416x416), YOLO predicts (52x52 + 26x26 13x13)x3 = 10647 bouding boxes.\n",
    "    - First, we filter boxes on theris objectness score.\n",
    "    - Then, Non-maximum Suppression\n",
    "        - Select only one box when several boxes overlap with each other and detect the same object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- box_confidence: contains pc\n",
    "- boxes: contains (bx, by, bh, bw)\n",
    "- box_class_probs: contain the detection probabilities (c1, ..., c80) for each 3 boxes per cell\n",
    "- Select the box that has the highest score.\n",
    "- Compute its overlap with all other boxes, and remove boxes that overlap it more than iou_threshold\n",
    "- Go back to step 1 and iterate until there's no more boxes with a lower score than the current selected box.\n",
    "- Applying Intersection over Union (IoU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Handle model\n",
    "import tensorflow.keras as keras\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "from matplotlib.patches import Rectangle \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from moviepy.editor import VideoFileClip, ImageClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert.py \n",
    "model = load_model('yolo.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, None, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, None, None, 1 432         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, None, 1 64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, None, None, 1 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, None, None, 1 0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, None, None, 3 4608        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, None, 3 128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, None, None, 3 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, None, None, 3 0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, None, None, 6 18432       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, None, 6 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, None, None, 6 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, None, None, 6 0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, None, None, 1 73728       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, None, None, 1 512         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, None, None, 1 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, None, None, 1 0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, None, None, 2 294912      max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, None, None, 2 1024        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, None, None, 2 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, None, None, 2 0           leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, None, None, 5 1179648     max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, None, None, 5 2048        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, None, None, 5 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, None, None, 5 0           leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, None, None, 1 4718592     max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, None, None, 1 4096        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, None, None, 1 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, None, None, 2 262144      leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, None, None, 2 1024        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, None, None, 2 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, None, None, 1 32768       leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, None, None, 1 512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, None, None, 1 0           leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, None, 3 0           up_sampling2d_1[0][0]            \n",
      "                                                                 leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, None, None, 5 1179648     leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, None, None, 2 884736      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, None, None, 5 2048        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, None, None, 2 1024        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, None, None, 5 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, None, None, 2 130815      leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, None, None, 2 65535       leaky_re_lu_11[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 8,858,734\n",
      "Trainable params: 8,852,366\n",
      "Non-trainable params: 6,368\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(image, net_h, net_w):\n",
    "    new_h, new_w, _ = image.shape\n",
    "\n",
    "    # determine the new size of the image\n",
    "    if (float(net_w)/new_w) < (float(net_h)/new_h):\n",
    "        new_h = (new_h * net_w)//new_w\n",
    "        new_w = net_w\n",
    "    else:\n",
    "        new_w = (new_w * net_h)//new_h\n",
    "        new_h = net_h\n",
    "\n",
    "    # resize the image to the new size\n",
    "    resized = cv2.resize(image[:,:,::-1]/255., (new_w, new_h))\n",
    "\n",
    "    # embed the image into the standard letter box\n",
    "    new_image = np.ones((net_h, net_w, 3)) * 0.5\n",
    "    new_image[(net_h-new_h)//2:(net_h+new_h)//2, (net_w-new_w)//2:(net_w+new_w)//2, :] = resized\n",
    "    new_image = np.expand_dims(new_image, 0)\n",
    "\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Decode Output of YOLOv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sigmoid(x):\n",
    "    # sigmoid for decoding the output\n",
    "    return 1./(1.+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _softmax(x, axis=-1):\n",
    "    x = x - np.amax(x, axis, keepdims=True)\n",
    "    e_x = np.exp(x)\n",
    "    \n",
    "    return e_x / e_x.sum(axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundBox:\n",
    "    def __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):\n",
    "        self.xmin = xmin\n",
    "        self.ymin = ymin\n",
    "        self.xmax = xmax\n",
    "        self.ymax = ymax\n",
    "        self.objness = objness\n",
    "        self.classes = classes\n",
    "        self.label = -1\n",
    "        self.score = -1\n",
    "        \n",
    "    def get_label(self):\n",
    "        if self.label == -1:\n",
    "            self.label = np.argmax(self.classes)\n",
    "        return self.label\n",
    "    \n",
    "    def get_score(self):\n",
    "        if self.score == -1:\n",
    "            self.score = self.classes[self.get_label()]\n",
    "        return self.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_netout(netout, anchors, obj_thresh, net_h, net_w):\n",
    "    # netout    : each output of the model - yolo3\n",
    "    # anchors   : anchors of the model - defined in .cfg \n",
    "    # obj_thresh: threshold prediction score of target object\n",
    "    # net_h     : input-height shape \n",
    "    # net_w     : input-width shape\n",
    "    \n",
    "    # height and width of features map\n",
    "    grid_h, grid_w = netout.shape[:2]\n",
    "    nb_box = 3\n",
    "    # each cell(anchor box) contains 3 boxes\n",
    "    netout = netout.reshape((grid_h, grid_w, nb_box, -1))\n",
    "    # number of classes used\n",
    "    nb_class = netout.shape[-1] - 5\n",
    "\n",
    "    boxes = []\n",
    "    # center and object are passed through sigmoid function.\n",
    "    netout[..., :2]  = _sigmoid(netout[..., :2])\n",
    "    netout[..., 4]   = _sigmoid(netout[..., 4])\n",
    "    # objectness score * confidence = probabilities\n",
    "    netout[..., 5:]  = netout[..., 4][..., np.newaxis] * _softmax(netout[..., 5:])\n",
    "    # threshold\n",
    "    netout[..., 5:] *= netout[..., 5:] > obj_thresh\n",
    "\n",
    "    for i in range(grid_h*grid_w):\n",
    "        row = i // grid_w\n",
    "        col = i % grid_w\n",
    "        \n",
    "        for b in range(nb_box):\n",
    "            # 4th element is objectness score\n",
    "            objectness = netout[row, col, b, 4]\n",
    "            \n",
    "            if(objectness <= obj_thresh): continue\n",
    "            \n",
    "            # first 4 elements are x, y, w, and h\n",
    "            x, y, w, h = netout[row,col,b,:4]\n",
    "\n",
    "            x = (col + x) / grid_w # center position, unit: image width\n",
    "            y = (row + y) / grid_h # center position, unit: image height\n",
    "            w = anchors[2 * b + 0] * np.exp(w) / net_w # unit: image width\n",
    "            h = anchors[2 * b + 1] * np.exp(h) / net_h # unit: image height  \n",
    "            \n",
    "            # last elements are class probabilities\n",
    "            classes = netout[row,col,b,5:]\n",
    "            \n",
    "            box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, objectness, classes)\n",
    "\n",
    "            boxes.append(box)\n",
    "\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):\n",
    "    if (float(net_w)/image_w) < (float(net_h)/image_h):\n",
    "        new_w = net_w\n",
    "        new_h = (image_h*net_w)/image_w\n",
    "    else:\n",
    "        new_h = net_w\n",
    "        new_w = (image_w*net_h)/image_h\n",
    "        \n",
    "    for i in range(len(boxes)):\n",
    "        x_offset, x_scale = (net_w - new_w)/2./net_w, float(new_w)/net_w\n",
    "        y_offset, y_scale = (net_h - new_h)/2./net_h, float(new_h)/net_h\n",
    "        \n",
    "        boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)\n",
    "        boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)\n",
    "        boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)\n",
    "        boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_nms(boxes, nms_thresh):\n",
    "    if len(boxes) > 0:\n",
    "        nb_class = len(boxes[0].classes)\n",
    "    else:\n",
    "        return\n",
    "        \n",
    "    for c in range(nb_class):\n",
    "        sorted_indices = np.argsort([-box.classes[c] for box in boxes])\n",
    "\n",
    "        for i in range(len(sorted_indices)):\n",
    "            index_i = sorted_indices[i]\n",
    "\n",
    "            if boxes[index_i].classes[c] == 0: continue\n",
    "\n",
    "            for j in range(i+1, len(sorted_indices)):\n",
    "                index_j = sorted_indices[j]\n",
    "\n",
    "                if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:\n",
    "                    boxes[index_j].classes[c] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color(label):\n",
    "    if label < len(colors):\n",
    "        return colors[label]\n",
    "    else:\n",
    "        print('Label {} has no color, returning default.'.format(label))\n",
    "        return (0, 255, 0)\n",
    "\n",
    "colors = [\n",
    "    [31  , 0   , 255] ,\n",
    "    [0   , 159 , 255] ,\n",
    "    [255 , 95  , 0]   ,\n",
    "    [255 , 19  , 0]   ,\n",
    "    [255 , 0   , 0]   ,\n",
    "    [255 , 38  , 0]   ,\n",
    "    [0   , 255 , 25]  ,\n",
    "    [255 , 0   , 133] ,\n",
    "    [255 , 172 , 0]   ,\n",
    "    [108 , 0   , 255] ,\n",
    "    [0   , 82  , 255] ,\n",
    "    [0   , 255 , 6]   ,\n",
    "    [255 , 0   , 152] ,\n",
    "    [223 , 0   , 255] ,\n",
    "    [12  , 0   , 255] ,\n",
    "    [0   , 255 , 178] ,\n",
    "    [108 , 255 , 0]   ,\n",
    "    [184 , 0   , 255] ,\n",
    "    [255 , 0   , 76]  ,\n",
    "    [146 , 255 , 0]   ,\n",
    "    [51  , 0   , 255] ,\n",
    "    [0   , 197 , 255] ,\n",
    "    [255 , 248 , 0]   ,\n",
    "    [255 , 0   , 19]  ,\n",
    "    [255 , 0   , 38]  ,\n",
    "    [89  , 255 , 0]   ,\n",
    "    [127 , 255 , 0]   ,\n",
    "    [255 , 153 , 0]   ,\n",
    "    [0   , 255 , 255] ,\n",
    "    [0   , 255 , 216] ,\n",
    "    [0   , 255 , 121] ,\n",
    "    [255 , 0   , 248] ,\n",
    "    [70  , 0   , 255] ,\n",
    "    [0   , 255 , 159] ,\n",
    "    [0   , 216 , 255] ,\n",
    "    [0   , 6   , 255] ,\n",
    "    [0   , 63  , 255] ,\n",
    "    [31  , 255 , 0]   ,\n",
    "    [255 , 57  , 0]   ,\n",
    "    [255 , 0   , 210] ,\n",
    "    [0   , 255 , 102] ,\n",
    "    [242 , 255 , 0]   ,\n",
    "    [255 , 191 , 0]   ,\n",
    "    [0   , 255 , 63]  ,\n",
    "    [255 , 0   , 95]  ,\n",
    "    [146 , 0   , 255] ,\n",
    "    [184 , 255 , 0]   ,\n",
    "    [255 , 114 , 0]   ,\n",
    "    [0   , 255 , 235] ,\n",
    "    [255 , 229 , 0]   ,\n",
    "    [0   , 178 , 255] ,\n",
    "    [255 , 0   , 114] ,\n",
    "    [255 , 0   , 57]  ,\n",
    "    [0   , 140 , 255] ,\n",
    "    [0   , 121 , 255] ,\n",
    "    [12  , 255 , 0]   ,\n",
    "    [255 , 210 , 0]   ,\n",
    "    [0   , 255 , 44]  ,\n",
    "    [165 , 255 , 0]   ,\n",
    "    [0   , 25  , 255] ,\n",
    "    [0   , 255 , 140] ,\n",
    "    [0   , 101 , 255] ,\n",
    "    [0   , 255 , 82]  ,\n",
    "    [223 , 255 , 0]   ,\n",
    "    [242 , 0   , 255] ,\n",
    "    [89  , 0   , 255] ,\n",
    "    [165 , 0   , 255] ,\n",
    "    [70  , 255 , 0]   ,\n",
    "    [255 , 0   , 172] ,\n",
    "    [255 , 76  , 0]   ,\n",
    "    [203 , 255 , 0]   ,\n",
    "    [204 , 0   , 255] ,\n",
    "    [255 , 0   , 229] ,\n",
    "    [255 , 133 , 0]   ,\n",
    "    [127 , 0   , 255] ,\n",
    "    [0   , 235 , 255] ,\n",
    "    [0   , 255 , 197] ,\n",
    "    [255 , 0   , 191] ,\n",
    "    [0   , 44  , 255] ,\n",
    "    [50  , 255 , 0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _interval_overlap(interval_a, interval_b):\n",
    "    x1, x2 = interval_a\n",
    "    x3, x4 = interval_b\n",
    "\n",
    "    if x3 < x1:\n",
    "        if x4 < x1:\n",
    "            return 0\n",
    "        else:\n",
    "            return min(x2,x4) - x1\n",
    "    else:\n",
    "        if x2 < x3:\n",
    "             return 0\n",
    "        else:\n",
    "            return min(x2,x4) - x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_iou(box1, box2):\n",
    "    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n",
    "    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])  \n",
    "    \n",
    "    intersect = intersect_w * intersect_h\n",
    "\n",
    "    w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n",
    "    w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n",
    "    \n",
    "    union = w1*h1 + w2*h2 - intersect\n",
    "    \n",
    "    return float(intersect) / union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image, boxes, labels, obj_thresh, quiet=True):\n",
    "    for box in boxes:\n",
    "        label_str = ''\n",
    "        label = -1\n",
    "        \n",
    "        for i in range(len(labels)):\n",
    "            if box.classes[i] > obj_thresh:\n",
    "                if label_str != '': label_str += ', '\n",
    "                label_str += (labels[i] + ' ' + str(round(box.get_score()*100, 2)) + '%')\n",
    "                label = i\n",
    "            if not quiet: print(label_str)\n",
    "                \n",
    "        if label >= 0:\n",
    "            text_size = cv2.getTextSize(label_str, cv2.FONT_HERSHEY_SIMPLEX, 1.1e-3 * image.shape[0], 5)\n",
    "            width, height = text_size[0][0], text_size[0][1]\n",
    "            region = np.array([[box.xmin-3,        box.ymin], \n",
    "                               [box.xmin-3,        box.ymin-height-26], \n",
    "                               [box.xmin+width+13, box.ymin-height-26], \n",
    "                               [box.xmin+width+13, box.ymin]], dtype='int32')  \n",
    "\n",
    "            cv2.rectangle(img=image, pt1=(box.xmin,box.ymin), pt2=(box.xmax,box.ymax), color=get_color(label), thickness=5)\n",
    "            cv2.fillPoly(img=image, pts=[region], color=get_color(label))\n",
    "            cv2.putText(img=image, \n",
    "                        text=label_str, \n",
    "                        org=(box.xmin+13, box.ymin - 13), \n",
    "                        fontFace=cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        fontScale=1e-3 * image.shape[0], \n",
    "                        color=(0,0,0), \n",
    "                        thickness=2)\n",
    "        \n",
    "    return image          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 2000, 3)\n",
      "(1, 416, 416, 3)\n",
      "Label:  car\n",
      "Score:  0.6363732\n"
     ]
    }
   ],
   "source": [
    "# show input image\n",
    "test_image = cv2.imread('test_image.jpg')\n",
    "print(test_image.shape)\n",
    "input_image = preprocess_input(test_image, 416, 416)\n",
    "print(input_image.shape)\n",
    "# plt.imshow(test_image[0])\n",
    "# Perform prediction on test image\n",
    "netout = model.predict(input_image)\n",
    "\n",
    "obj_thresh = 0.5\n",
    "anchors = [[10,14,  23,27,  37,58],  [81,82,  135,169,  344,319]]\n",
    "# print(\"Number of boxes in\")\n",
    "# res_0 = decode_netout(netout[0][0], anchors[0], 0.5, 416, 416)\n",
    "# print(\"Large scale:  \", len(res_0))\n",
    "# res_1 = decode_netout(netout[1][0], anchors[1], 0.5, 416, 416)\n",
    "# print(\"Medium scale: \", len(res_1))\n",
    "boxes = []\n",
    "\n",
    "for i in range(len(netout)):\n",
    "    boxes += decode_netout(netout[i][0], anchors[i], obj_thresh, 416, 416)\n",
    "correct_yolo_boxes(boxes, 386, 640, 416, 416)\n",
    "do_nms(boxes, 0.5)\n",
    "\n",
    "labels = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\",\n",
    "    \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\",\n",
    "    \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\",\n",
    "    \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\",\n",
    "    \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n",
    "    \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\",\n",
    "    \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\",\n",
    "    \"chair\", \"sofa\", \"pottedplant\", \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\",\n",
    "    \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\",\n",
    "    \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]\n",
    "for box in boxes:\n",
    "    print(\"Label: \", labels[box.get_label()])\n",
    "    print(\"Score: \", box.get_score())\n",
    "    \n",
    "rs_image = draw_boxes(test_image, boxes, labels, obj_thresh)\n",
    "cv2.imshow('sth', rs_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum up everything to 1 functions\n",
    "def predict(image, obj_thresh):\n",
    "    image_w, image_h, _ = image.shape\n",
    "    # image: read by using cv2.imread\n",
    "    netout = model.predict(image)\n",
    "    # anchors of yolov3-tiny model\n",
    "    anchors = [[10,14,  23,27,  37,58],  [81,82,  135,169,  344,319]]\n",
    "    # get bouding boxes\n",
    "    boxes = []\n",
    "    for i in range(len(netout)):\n",
    "        boxes += decode_netout(netout[i][0], anchors[i], obj_thresh, 416, 416)\n",
    "    correct_yolo_boxes(boxes, image_w, image_h, 416, 416)\n",
    "    # non-maximum suppression\n",
    "    do_nms(boxes, 0.5)\n",
    "    # labels follow coco dataset\n",
    "    labels = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\",\n",
    "        \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\",\n",
    "        \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\",\n",
    "        \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\",\n",
    "        \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n",
    "        \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\",\n",
    "        \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\",\n",
    "        \"chair\", \"sofa\", \"pottedplant\", \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\",\n",
    "        \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\",\n",
    "        \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]\n",
    "    # details of predicted values of model\n",
    "    for box in boxes:\n",
    "        print(\"Label: \", labels[box.get_label()], \"Score: \", box.get_score())\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'photo_filename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-c7c8602f1a4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# draw what we found\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdraw_boxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphoto_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_boxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'photo_filename' is not defined"
     ]
    }
   ],
   "source": [
    "# draw what we found\n",
    "draw_boxes(photo_filename, v_boxes, v_labels, v_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Apply YOLOv3 to video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output video from sequence of images\n",
    "from moviepy.editor import *\n",
    "img = []\n",
    "\n",
    "clips = [ImageClip(m).set_duration(2) for m in img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output video from video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response to the Event "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decided based on \n",
    "- Center and area of the bouding box.\n",
    "- Speed estimation of the detected object. \n",
    "\n",
    "\n",
    "Output can be: Straight, Left, Right, Stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
